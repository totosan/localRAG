# Copilot Instructions for localRAG

1. **Purpose & perspective**: localRAG is a learning-focused, local RAG assistant built on Semantic Kernel + Kernel Memory. `Program.cs` orchestrates CLI flags (`--ollama`, `--import`, `--index`), `tags.json` management, kernel/memory wiring, and the multi-step process graph (`ChatUserInputStep` → `RewriteAskStep` → `LookupKernelmemoriesStep` → `ResponseStepWithHalluCheck`). Read it top-to-bottom to understand how the UI/CLI, ingestion guardrails, and process builder flow tie together before modifying higher-level behavior. Re-run the same flow with different flag combinations to demonstrate how toggling features changes the ingestion, retrieval, and hallucination guard rails.

2. **Memory connectors**: `Utilities/Helpers.cs` exposes `GetMemoryConnector<T>`; it branches into: remote `MemoryWebClient`, Azure-backed builders, or the local path that mirrors the Kernel Memory serverless example (Ollama + `KernelMemoryBuilder`). The local branch consistently sets up `WithSimpleFileStorage("tmp-data")`, `WithSimpleVectorDb("tmp-data")`, `CustomPdfDecoder`, and tag generation handlers—follow that pattern unless you intentionally replace storage/embedding backends.

3. **Persistence & re-index guardrails**: Document ingestion happens via `LongtermMemoryHelper.LoadAndStorePdfFromPathAsync`, taking cues from Kernel Memory’s example pipeline. Embeddings/cache go into `tmp-data/`; `Program.cs` skips work when `tags.json` exists and `--import`/`--index` flags are absent. Any changes that affect the import path, storage folders, or tag logic should keep that guard/flag behavior intact so `tags.json` remains the source of truth for incremental runs.

4. **Handler & prompt plumbing**: `GenerateTagsHandler` (PipelineHandler) produces `DocumentCategory` tags and drives prompts in `Plugins/Prompts` (see subfolders for each template). Handlers must be registered via `memoryConnector.Orchestrator.AddHandler(...)` with the same signature as in `Program.cs`, otherwise the process builder will hang. When you wire new handlers, update the prompts directory, `tags.json` generation, and `GenerateTagsHandler` constructor usage simultaneously.

5. **Run variations**: Follow the Semantic Kernel + Kernel Memory quick-start instructions from the upstream docs: `dotnet build` to compile; `dotnet run -- --ollama` to exercise the local flow; `dotnet run -- --ollama --import`/`--index` to force re-ingestion. Each run uses `.env` (see `env.example`) to supply `IMPORT_PATH`, `TAGS_COLLECTION_FILE`, `AZURE_*`, and `OLLAMA_*`. Use `Helpers.EnvVar` when reading new env keys to keep the failure semantics consistent.

6. **Learning abstractions**: To teach RAG, emphasize how documents are normalized, chunked, and keyword-enriched before retrieval. Point out `Decoder/CustomPdfDecoder.cs` (document normalization and chunk creation), `Utilities/LongtermMemoryHelper.cs` (chunk size control during `ExtractPageChunksAsync`), `Utilities/KeywordExtractor.cs` (rule-based keyword extraction using TF-IDF + RAKE), and `PipelineHandler/GenerateTagsHandler.cs` (two-tier tagging: fast keyword extraction + semantic LLM categorization powering `tags.json`). Explain that chunk metadata stored via `Chunk.Meta(...)` carries page numbers, sentence boundary hints, and keywords that downstream retrieval relies on. Keywords are extracted using multiple methods: frequency analysis (TF-IDF-inspired), technical term detection, key phrase extraction (RAKE-inspired), and named entity recognition. You can also demonstrate Docling running inside the provided Docker container to show an alternate normalization+chunking pipeline; compare Docling exports with our `tmp-data` chunks to highlight abstraction boundaries (Docling outputs structured JSON/DocTags, while we emit `Chunk` sections).

7. **Chunking & semantic strategy**: The KM documentation demonstrates pipelines that chunk documents into overlapping sections. Mirror that by describing how `GenerateTagsHandler` processes `DocumentCategory` entries and how you can tune chunk size/semantic boundaries in `LongtermMemoryHelper` or by adding new `Chunk` constructors with custom metadata (e.g., sentencesComplete, page numbers). Mention that chunking drives both retrieval recall and reranking.

8. **Retrieval process**: `LookupKernelmemoriesStep` sits between rewritten user asks and vector search. It calls `LongtermMemoryHelper` to run `memory.AskAsync(...)`, collects matching `Chunk` sections, and forwards them to `ResponseStepWithHalluCheck`. Explain how `HallucinationCheckPlugin` uses retrieved chunks for context vs. the assistant output, enabling reranking or cautionary warnings.

9. **Optimization cheat sheet**: Use this doc to capture advanced knobs. Each feature can be toggled when presenting: keep `--ollama --import` on for indexing, rerun without `--import` to show cold vs. warm caches, and turn the hallucination guard in `ResponseStepWithHalluCheck` on/off by skipping the plugin.
    - **Hybrid search**: Implemented via metadata-enhanced retrieval combining semantic vector search with keyword-based filtering. `KeywordExtractor.cs` provides rule-based extraction (TF-IDF + RAKE + named entities), while `GenerateTagsHandler` stores keywords in `pipeline.Tags["keywords"]` for filtering during `SearchAsync`. This approach combines fast deterministic keyword matching with semantic embeddings—functionally similar to BM25+vector fusion but optimized for debugging and education. For production scale, swap to `.WithAzureAISearchMemoryDb()` which provides native BM25 inverted indexes. See `HYBRID_SEARCH_DEMO.md` for presentation talking points.
    - **Reranking**: Implemented in `Utilities/Reranker.cs` and automatically applied in `LongtermMemoryHelper.GetLongTermMemory()` after initial retrieval. Uses embedding-based semantic similarity to re-score documents against the user query, then blends reranking scores (70%) with original retrieval scores (30%) for optimal relevance. The reranker generates embeddings for both query and each document chunk, computes cosine similarity, and re-sorts results. Provides visible logging (`[Reranker] Doc: file.pdf | Original: 0.450 | Rerank: 0.892 | Blended: 0.759`) perfect for live demos. Works with both Azure OpenAI and Ollama embeddings via `Helpers.GetEmbeddingGenerator()`. Also includes a simpler keyword-based fallback (`ReRankByKeywordOverlap`) for teaching purposes. See `RERANKING_DEMO.md` for presentation talking points and demo scripts.
    - **Hallucination prevention**: Highlight `HallucinationCheckPlugin` (and the guard in `ResponseStepWithHalluCheck`) as the built-in pattern. When adding features, keep this check in mind and consider injecting additional context chunks or fallback fact-check prompts if flagged.

10. **Local RAG showcase tips**: To present this locally (e.g., at a conference), highlight the Everett-style control flow: CLI input → rewrite intent → memory lookup → hallucination check. Show how `tags.json` + `tmp-data/` allow fast subsequent runs (lost data only when you run `--import`). Use the `HallucinationCheckPlugin` flow inside `ResponseStepWithHalluCheck` to explain the guard rails and the `GenerateTagsHandler` / `Plugins/Prompts` pairing for synthetic questions.

11. **Semantic Kernel + Kernel Memory best practices**: Always import prompt directories with `kernel.ImportPluginFromPromptDirectory(...)` so prompts stay centralized. When adjusting pipelines, mimic Kernel Memory’s handler pattern: optional `WithoutDefaultHandlers()`, plug in custom handlers (see `GenerateTagsHandler`) and use `MemoryServerless` for sync flows. When you change prompt templates or Handler steps, keep `tags.json`, `Plugins/Prompts`, and handler constructors synchronized.

12. **Possible extensions**:
    - Custom decoders should implement `Microsoft.KernelMemory.DataFormats.IContentDecoder` (see `Decoder/CustomPdfDecoder.cs`). Persisted output must add `Chunk` sections and metadata. This mirrors KM’s ingestion pipeline docs (chunk → embed → store).
    - For new LLMs or connectors, reuse `Helpers.GetSemanticKernel` and `GetMemoryConnector` instead of instantiating kernels manually, to keep logging, chat history, and telemetry consistent.

6. **Process-step reliability**: `StepRewriteAsk` deserializes JSON from `Plugins/Prompts/RewriteUserAskPlugin`. When the prompt output is invalid (e.g., HTML or `<`), fallback to `userInput` so `SearchData.StandaloneQuestions.First()` never throws. The same notes apply to `ResponseStepWithHalluCheck`: it depends on `_kernel.GetHistory()` + `IChatCompletionService` and emits `KernelProcessEvent`s (see `Process/Steps`). Preserve those event flows if you refactor the process graph.

7. **Flexible architecture**: The upstream Kernel Memory repo shows you can swap storage, embeddings, and connectors without touching the rest of the workflow. You can mirror that by adding new `With*` builder calls in `Helpers.GetMemoryConnector`, registering custom handlers, or replacing `tmp-data/` with another folder (just update `SimpleFileStorage`/`SimpleVectorDb` calls and environment variables accordingly). Treat file/folder layouts (e.g., moving `Decoder/` or `PipelineHandler/`) as replaceable if your alternative design preserves the core RAG goal: ingest, index, rewrite, and respond locally.

8. **Local RAG showcase tips**: To present this locally (e.g., at a conference), highlight the Everett-style control flow: CLI input → rewrite intent → memory lookup → hallucination check. Show how `tags.json` + `tmp-data/` allow fast subsequent runs (lost data only when you run `--import`). Use the `HallucinationCheckPlugin` flow inside `ResponseStepWithHalluCheck` to explain the guard rails and the `GenerateTagsHandler` / `Plugins/Prompts` pairing for synthetic questions. Because you’re speaking at .NET Conf, keep all examples, snippets, and rewrite plans in C#/.NET. If you refer to Semantic Kernel or Kernel Memory docs written in other languages, plan to translate those snippets into C# before showing them.

9. **Semantic Kernel + Kernel Memory best practices**: Always import prompt directories with `kernel.ImportPluginFromPromptDirectory(...)` so prompts stay centralized. When adjusting pipelines, mimic Kernel Memory’s handler pattern: optional `WithoutDefaultHandlers()`, plug in custom handlers (see `GenerateTagsHandler`) and use `MemoryServerless` for sync flows. When you change prompt templates or Handler steps, keep `tags.json`, `Plugins/Prompts`, and handler constructors synchronized.

10. **Possible extensions**:
    - Custom decoders should implement `Microsoft.KernelMemory.DataFormats.IContentDecoder` (see `Decoder/CustomPdfDecoder.cs`), emitting `Chunk` sections and metadata. This mirrors KM’s ingestion pipeline docs (chunk → embed → store).
    - For new LLMs or connectors, reuse `Helpers.GetSemanticKernel` and `GetMemoryConnector` instead of instantiating kernels manually, to keep logging, chat history, and telemetry consistent.

If you plan to rearrange folders or plug in different storage/LLMs, keep documenting the intent in this file so future copilots quickly know where the RAG loop now lives. Let me know if any section feels incomplete so we can refine it before your talk.